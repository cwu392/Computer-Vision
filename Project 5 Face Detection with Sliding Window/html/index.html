<html>
<head>
<title>Face Detection Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Chiamin Wu </h1>
</div>
</div>
<div class="container">

<h2> Project 5 / Face Detection with a Sliding Window</h2>

<div style="float: right; padding: 20px">
<img src="2017_class_easy_result.jpg" width="70%" />
<p style="font-size: 14px">Example Result of 2018 Computer Vision Class.</p>
</div>
<h2> Outline:</h2>
<p> 	In this project, we use HoG and SVM to create a multi-scale human face detector.</p>

<ol>
<li>Collect Positive 36x36-HumanFace Data From Caltech CropFace Dataset.</li>
<li>Random Collect Haphazard Negative Data From SUN Dataset</li>
<li>Train Linear SVM Classifier</li>
<li>Use SVM Classifier to Create a muilt-Scale Human Face Detector</li>
<li>Extra Credit</li>
</ol>

<div style="clear:both">
<h3>1. Collect Positive 36x36-HumanFace Data From Caltech CropFace Dataset</h3>

<p> 	To extract positive training data, we browse 6,713 cropped 36x36 faces from Caltech's face dataset. Then, by using VL_hog in VL_feat, we retrieve Histogram of Gradient(HoG) features to represent faces. To improve performance, we can flip images from left to right to augment our positive training data.</p>

<h3>Code Example:</h3>
<pre><code>D=temp_size/cell_size)^2*31;
for i=1:2*num_images
    img = imread(fullfile(image_files(ceil(i/2)).folder,image_files(ceil(i/2)).name));
    img=single(img)/255;
    if(size(img,3) > 1)
        img = rgb2gray(img);
    end   
    if mod(i, 2) == 0
    	%If i%2==0: flip image
        hog=vl_hog(fliplr(img),cell_size,'verbose');
    else
        hog=vl_hog(img,cell_size,'verbose');
    end
    features_pos(i,:)=reshape(hog,[1,D]);
end
</code></pre>

<h3>HoG Results:</h3>
<div align="center">
<table border=1>
<tr>
<td> cell size = 2 </td>
<td> cell size = 4 </td>
<td> cell size = 6 </td>
</tr>
<tr>
<td> <img src="../code/Cell_Size_3.jpg" width="100%"/> </td>
<td> <img src="../code/Cell_Size_4.jpg" width="100%"/> </td>
<td> <img src="../code/Cell_Size_6.jpg" width="100%"/> </td>
</tr>
</table>
</div>
<h3>2. Random Collect Haphazard Negative Data From SUN Dataset</h3>

<p> 	To extract negative training data, we browse 275 Non-Face images from SUN dataset. Like positive features, we use VL_hog to retrieve Histogram of Gradient(HoG) and use it to represent our non-face features. To keep the same dimensionality, we align our cropping window the same as the positive feature's template size (which is 36x36). To increase training data, we random crop windows within 275 images and generate over 10,000 negative training examples.</p>
<h3>Code Example:</h3>
<pre><code>D=(temp_size/cell_size)^2*31;
k=1; %Total
scale_factor=0.9;

for i=1:num_images
    img = imread(fullfile(image_files(i).folder, image_files(i).name ));
    img = single(img)/255;
    if (size(img,3)>1)
        img = rgb2gray(img);
    end

    num_of_feat=floor(size(img,1)/temp_size)*floor(size(img,2)/temp_size);
    while (size(img,1)>=temp_size) && (size(img,2)>=temp_size) && (k<=num_samples)
        for j = 1 : num_of_feat
            x = ceil(rand() * (size(img,1) - temp_size)) + 1;
            y = ceil(rand() * (size(img,2) - temp_size)) + 1;
            %Random crop image into 36x36 box: 
            box = img(x : x + temp_size - 1, y : y + temp_size - 1);
            hog = vl_hog(single(box), cell_size);
            hog = reshape(hog,[1,D]);
            features_neg(k, :) = hog;
            k = k + 1;
        end
        img = imresize(img, scale_factor);
        num_of_feat=floor(size(img,1)/temp_size)*floor(size(img,2)/temp_size);
    end    
end
</code></pre>
<h3>3. Train Linear SVM Classifier</h3>
<p> 	In this part, we use function vl_svmtrain to process our positive/negative training data. With properly selected lambda, we can get accuracy 100% with True Positive Ratio: 57.2% and False Negative: 42.8%.</p>
<h3>Code Example:</h3>
<pre><code>lambda=0.00001;
%Training Data:
	X_pos=features_pos;
	X_neg=features_neg;
%Training Label:
	X_pos_label=ones(size(X_pos,1),1);
	X_neg_label=-ones(size(X_neg,1),1);
%Collect Data to SVM:
	X=vertcat(X_pos,X_neg);
	Y=vertcat(X_pos_label,X_neg_label);
	%X DxN matrix, Y N element
	[w b]=vl_svmtrain(X',Y,lambda);
</code></pre>

<h3>SVM Training Result:</h3>
<div align="center">
<table border=1>
<tr>
<td> <img src="./SVM_Training_Result.jpg" width="100%"/> </td>
</tr>
</table>
</div>
<h3>4. Use SVM Classifier to Create a muilt-Scale Human Face Detector</h3>
<p> 	In this part, we use linear SVM to detect all possible faces in our image. At first, we create non-overlapping boxes in our input image. Then, we transfer each box into HoG representation. By using SVM model from Step 3, we can get the confidence level of each box. After collecting all boxes at all scale, we use non-maxima suppression to remove repeated boxes and preserve the most confident ones.</p>
<p> 	Under single-scale condition, we first get accuracy about 39.2%, which is because our testing data does not always have the same face size as our training examples. Then, we downscale our negative training examples and testing data with scale factor 0.9 in each iteration. After this, we can get accuracy over 83%. By flipping our positive training faces and double our positive training, we can improve about 3% accuracy. Ater adjust cell size to 3 and implement hard negative mining, we can achieve accuracy up to 93.4%.</p>
<h3>Code Example: Face Detection with Multi-Scale Sliding Windows</h3>
<pre><code>while (size(img,1)>temp_size) && (size(img,2)>temp_size)
    hog=vl_hog(img, cell_size);
    for y=1:(size(hog,1)-(temp_size/cell_size))
        for x=1:(size(hog,2)-(temp_size/cell_size))
            hog_feat = hog(y:(y+(temp_size/cell_size)-1), x:(x+(temp_size/cell_size)-1),:); %hog(1:36,1:36,[1:31])
            hog_feat = reshape(hog_feat,[1,D]);
            conf = hog_feat * w + b;
            if (conf > threshold)
                x_min = (x-1)*cell_size;
                y_min = (y-1)*cell_size;
                x_max = x_min + temp_size;
                y_max = y_min + temp_size;
                box = round([x_min, y_min, x_max, y_max] * scale);
                cur_bboxes      = [cur_bboxes;      box];
                cur_confidences = [cur_confidences; conf];
                cur_image_ids   = [cur_image_ids;   test_scenes(i).name];
            end
        end
    end
    img = imresize(img, scale_factor);
    scale=scale/scale_factor;
end

%Run Non-Max Suppression:
[is_maximum] = non_max_supr_bbox(cur_bboxes, cur_confidences, img_size);
cur_confidences = cur_confidences(is_maximum,:);
cur_bboxes      = cur_bboxes(     is_maximum,:);
cur_image_ids   = cur_image_ids(  is_maximum,:);
%Collect Data:
bboxes      = [bboxes;      cur_bboxes];
confidences = [confidences; cur_confidences];
image_ids   = [image_ids;   cur_image_ids];
</code></pre>
<h3>5. Extra Credit</h3>
<h4>5.1:Implement Hard Negative Mining (+10%)</h4>
<p> 	In hard negative mining, we adjust our run_detector to retrieve non-face features with high confidence. Because our estimation should be within [-2,2], the optimum goal of hard negative mining is to remove features which are not faces but with high confidence in our primal SVM classifier. To avoid overfitting, we can still employ maximum suppression. After adding this function, we can improve our accuracy from 88.3 to 88.9(+1.6%) under cell size=6.</p>
<h5>Code Example:</h5>
<pre><code>threshold=2.0;
for i = 1:length(test_scenes)
    features=[];
    
    while (size(img,1)>temp_size) && (size(img,2)>temp_size)
        hog=vl_hog(img, cell_size);
        for y=1:(size(hog,1)-(temp_size/cell_size))
            for x=1:(size(hog,2)-(temp_size/cell_size))
                hog_feat = hog(y:(y+(temp_size/cell_size)-1), x:(x+(temp_size/cell_size)-1),:); %hog(1:36,1:36,[1:31])
                hog_feat = reshape(hog_feat,[1,D]);
                conf = hog_feat * w + b;
                if (conf > threshold)
                    x_min = (x-1)*cell_size;
                    y_min = (y-1)*cell_size;
                    x_max = x_min + temp_size;
                    y_max = y_min + temp_size;
                    box = round([x_min, y_min, x_max, y_max] * scale);
                    cur_bboxes      = [cur_bboxes;      box];
                    cur_confidences = [cur_confidences; conf];
                    cur_image_ids   = [cur_image_ids;   test_scenes(i).name];
                    features        = [features; hog_feat];
                end
            end
        end
        img = imresize(img, scale_factor);
        scale=scale/scale_factor;
    end
    [is_maximum] = non_max_supr_bbox(cur_bboxes, cur_confidences, img_size);

    cur_confidences = cur_confidences(is_maximum,:);
    cur_bboxes      = cur_bboxes(     is_maximum,:);
    cur_image_ids   = cur_image_ids(  is_maximum,:);
    features        = features(       is_maximum,:);
    
    bboxes      = [bboxes;      cur_bboxes];
    confidences = [confidences; cur_confidences];
    image_ids   = [image_ids;   cur_image_ids];
    mhn_features= [mhn_features;features];
end
</code></pre>
<h4>5.2:Find and Utilize Alternative Positive Training Data: ORL Dataset from AT&T (+10%)</h4>
<p>	In this part, I add "The ORL Database of Faces" from AT&T Lab at Cambridge University. The main reason I choose this dataset is I found there are some tilting heads in our extra-credit images. In this case, ORL dataset is composed of 400 faces with different angles. After experiment, this dataset can help me improve 0.6% performance on MIT+CMU dataset.</p>
<div align="center">
<table border=1>
<tr>
<td> <img src="./att_faces/s1_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s1_10.jpg" width="80%"/> </td>
</tr>
<tr>
<td> <img src="./att_faces/s2_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s2_10.jpg" width="80%"/> </td>
</tr>

<tr>
<td> <img src="./att_faces/s3_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s3_10.jpg" width="80%"/> </td>
</tr>
<tr>
<td> <img src="./att_faces/s4_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s4_10.jpg" width="80%"/> </td>
</tr>
<tr>
<td> <img src="./att_faces/s11_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s11_10.jpg" width="80%"/> </td>
</tr>
<tr>
<td> <img src="./att_faces/s12_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s12_10.jpg" width="80%"/> </td>
</tr>

<tr>
<td> <img src="./att_faces/s13_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s13_10.jpg" width="80%"/> </td>
</tr>
<tr>
<td> <img src="./att_faces/s14_1.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_2.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_3.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_4.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_5.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_6.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_7.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_8.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_9.jpg" width="80%"/> </td>
<td> <img src="./att_faces/s14_10.jpg" width="80%"/> </td>
</tr>
</table>
</div>

<h4>5.3:Additional Classification:Nearest Neighbor (+10%)</h4>
<h5>Code Example:Nearest Neighbor</h5>
<pre><code>function conf = Nearest_Neighbor(X, Y, hog_feat)
[m,n]=size(X);
row_min=0;
dist=zeros(m,1);
for i=1:m
    dist(i)=sqrt(sum(abs(X(i,:)-hog_feat))); %RMS distance
end

row_min=min(dist);

for i=1:m
    if dist(i)==row_min
        conf=Y(i);
    end
end
</code></pre>

<h4>5.4:Run Bonus Scenes</h4>
<table border=1>
<tr>
<td> <img src="../code/visualizations/detections_cs143_2013_class_easy_01.jpg.png" width="500" height="400"/> </td>
<td> <img src="../code/visualizations/detections_cs143_2013_class_easy_02.jpg.png" width="500" height="400"/> </td>
</tr>
<tr>
<td> <img src="../code/visualizations/detections_cs143_2011_class_easy.jpg.png" width="500" height="360"/> </td>	
<td> <img src="../code/visualizations/detections_cs143_2011_class_hard.jpg.png" width="500" height="400"/> </td>
</tr>
<tr>
<td> <img src="../code/visualizations/detections_4476_2017_class_easy.jpg.png" width="500" height="400"/> </td>	
<td> <img src="../code/visualizations/detections_4476_2017_class_hard.jpg.png" width="500" height="400"/> </td>
</tr>
<tr>
<td> <img src="../code/visualizations/detections_4476_2016_class_easy.jpg.png" width="500" height="400"/> </td>
<td> <img src="../code/visualizations/detections_4476_2016_class_hard.jpg.png" width="500" height="400"/> </td>
</tr>
<tr>
<td> <img src="../code/visualizations/detections_cs143_2013_class_hard_01.jpg.png" width="500" height="610" /> </td>
<td> <img src="../code/visualizations/detections_cs143_2013_class_hard_03.jpg.png" width="500" height="400"/> </td>
</tr>
</table>


<h3>Face Detection Results</h3>

<div align="center">
<table border=1>

<tr>
<td> Cell Size </td>
<td> Precision</td>
<td> Viola Jones</td>
<td> Example Image</td>
</tr>

<tr>
<td> cell size = 6</td>
<td> 89.5% <img src="../code/visualizations/CellSize6_89p5/average_precision.png" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize6_89p5/violajones.jpg" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize6_89p5/detections_Argentina.jpg.png" width="350"/> </td>
</tr>

<tr>
<td> cell size = 4</td> 
<td> 92.6% <img src="../code/visualizations/CellSize4_92p6/average_precision.png" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize4_92p6/violajones.jpg" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize4_92p6/detections_Argentina.jpg.png" width="350"/> </td>
</tr>

<tr>
<td> cell size = 3</td>
<td> 92.9% <img src="../code/visualizations/CellSize3_92p9/average_precision.png" width="100%"/> </td>
<td> <img src="../code/visualizations/CellSize3_92p9/violajones.jpg" width="100%"/> </td>
<td> <img src="../code/visualizations/CellSize3_92p9/detections_Argentina.jpg.png" width="100%"/> </td>
</tr>
</tr>

<tr>
<td> cell size = 3 with Hard Negative Mining</td> 
<td> 93.4% <img src="../code/visualizations/CellSize3_93p4/average_precision.png" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize3_93p4/violajones.jpg" width="350"/> </td>
<td> <img src="../code/visualizations/CellSize3_93p4/detections_Argentina.jpg.png" width="350"/> </td>
</tr>



</table>
</div>

<h4>Reference:</h4>
<h5>ORL Database:http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</h5>

</body>
</html>
